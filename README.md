# GenAI Project

A modern AI-powered API service built with FastAPI, supporting both cloud-based (OpenAI) and local model inference.

## Features

- üöÄ **Fast & Modern**: Built with FastAPI for high performance
- ü§ñ **Flexible AI Backend**: Support for OpenAI API and local models (TinyLlama)
- üíª **CPU-Only Local Inference**: Run models without GPU (2-3GB RAM)
- üì° **Streaming Support**: Real-time SSE streaming responses
- üîß **Easy Configuration**: Environment-based configuration
- üê≥ **Docker Ready**: Containerized deployment support
- üéØ **Type Safe**: Full type hints with Pydantic validation
- üõ°Ô∏è **Error Handling**: Unified error response format with custom exceptions
- ‚è±Ô∏è **Timeout & Retry**: Automatic retry with exponential backoff for API calls
- üö¶ **Rate Limiting**: IP-based rate limiting to prevent API abuse
- ‚úÖ **Input Validation**: Comprehensive parameter validation with detailed error messages

## Project Structure

```
genai-project/
‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îú‚îÄ‚îÄ api/                    # API service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py         # FastAPI application
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py   # Custom exception classes
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes/         # API routes
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ openai_routes.py  # OpenAI API endpoints
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ local_routes.py   # Local model endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile          # Docker configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt    # API dependencies
‚îÇ   ‚îú‚îÄ‚îÄ rag/                    # RAG service (future)
‚îÇ   ‚îî‚îÄ‚îÄ agents/                 # AI agents (future)
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml      # Docker Compose configuration
‚îú‚îÄ‚îÄ eval/
‚îÇ   ‚îî‚îÄ‚îÄ cases/                  # Evaluation test cases
‚îú‚îÄ‚îÄ reports/                    # Test reports
‚îú‚îÄ‚îÄ .env.example                # Environment variables template
‚îú‚îÄ‚îÄ requirements.txt            # Production dependencies
‚îú‚îÄ‚îÄ requirements-dev.txt        # Development dependencies (optional)
‚îú‚îÄ‚îÄ requirements-local.txt      # Local model dependencies (optional)
‚îî‚îÄ‚îÄ README.md                   # This file
```

## Quick Start

### Prerequisites

- Python 3.13+
- pip
- (Optional) Docker & Docker Compose

### Installation

#### 1. Clone the repository

```bash
git clone https://github.com/yourusername/genai-project.git
cd genai-project
```

#### 2. Create virtual environment

```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Linux/Mac
python -m venv .venv
source .venv/bin/activate
```

#### 3. Install dependencies

**Basic installation (OpenAI API only):**

```bash
pip install -r requirements.txt
```

**With local models (optional):**

```bash
pip install -r requirements-local.txt
```

**With development tools (optional, for testing):**

```bash
pip install -r requirements-dev.txt
```

**Install all dependencies:**

```bash
pip install -r requirements.txt -r requirements-dev.txt -r requirements-local.txt
```

#### 4. Configure environment

```bash
# Copy example environment file
cp .env.example .env

# Edit .env and add your API keys
# OPENAI_API_KEY=your_key_here
```

#### 5. Run the application

**Development mode:**

```bash
uvicorn apps.api.src.main:app --reload --host 0.0.0.0 --port 8000
```

**Using build script (Windows):**

```powershell
.\build.ps1 dev-api
```

The API will be available at `http://localhost:8000`

- **API Documentation**: http://localhost:8000/docs
- **Alternative Docs**: http://localhost:8000/redoc

## API Endpoints

### Health Check

```bash
GET /health
GET /healthz
```

Response:
```json
{
  "status": "ok"
}
```

### Generate Text

Unified endpoint supporting both streaming and non-streaming responses.

**Endpoint:** `POST /generate`

**Query Parameters:**
- `stream` (boolean, optional): Enable SSE streaming (default: false)

**Request Body:**

```json
{
  "prompt": "Explain artificial intelligence",
  "temperature": 0.7,
  "top_p": 0.9,
  "max_tokens": 500
}
```

**Response (Non-streaming):**

```json
{
  "response": "Artificial intelligence is..."
}
```

**Response (Streaming with `?stream=true`):**

Server-Sent Events (SSE) format:

```
data: Artificial
data: intelligence
data: is
...
```

### Examples

**Non-streaming request:**

```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is Python?",
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 500
  }'
```

**Streaming request:**

```bash
curl --no-buffer -X POST "http://localhost:8000/generate?stream=true" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Write a poem about AI",
    "temperature": 1.2
  }'
```

**Using PowerShell:**

```powershell
$body = @{
    prompt = "Hello, AI!"
    temperature = 0.7
} | ConvertTo-Json

Invoke-RestMethod -Uri "http://localhost:8000/generate" `
  -Method Post `
  -ContentType "application/json" `
  -Body $body
```

### Local Model Generation

Run AI models locally on your machine (requires `requirements-local.txt`).

**Endpoint:** `POST /local/generate`

**Request Body:**

```json
{
  "prompt": "What is Python?",
  "temperature": 0.0,
  "top_p": 0.9,
  "max_tokens": 100
}
```

**Parameters:**
- `prompt` (string, required): The text prompt to generate from
- `temperature` (float, optional): 0.0 for deterministic, >0 for creative (default: 0.0)
- `top_p` (float, optional): Nucleus sampling parameter (default: 0.9)
- `max_tokens` (int, optional): Maximum tokens to generate (default: 500)

**Response:**

```json
{
  "response": "Python is a high-level programming language designed for readability and simplicity. It supports multiple programming paradigms including procedural, object-oriented, and functional programming..."
}
```

**Example with curl (Windows):**

```powershell
curl.exe -X POST "http://localhost:8000/local/generate" -H "Content-Type: application/json" -d '{\"prompt\": \"What is Python?\", \"max_tokens\": 100, \"temperature\": 0.0}'
```

#### Streaming Mode

Enable real-time streaming output by adding `?stream=true` query parameter.

**Endpoint:** `POST /local/generate?stream=true`

**Request (same as above):**

```json
{
  "prompt": "Write a short story",
  "temperature": 0.7,
  "max_tokens": 100
}
```

**Response (Server-Sent Events):**

Text chunks are streamed in real-time as they are generated:

```
Once
 upon
 a
 time
,
 there
 was
...
```

**Example with curl:**

```powershell
curl.exe --no-buffer -X POST "http://localhost:8000/local/generate?stream=true" -H "Content-Type: application/json" -d '{\"prompt\": \"Tell me a joke\", \"max_tokens\": 50, \"temperature\": 0.7}'
```

**Example with Python (requires `requests`):**

```python
import requests

response = requests.post(
    "http://localhost:8000/local/generate?stream=true",
    json={"prompt": "Hello", "max_tokens": 50, "temperature": 0.7},
    stream=True
)

for chunk in response.iter_content(chunk_size=1, decode_unicode=True):
    if chunk:
        print(chunk, end='', flush=True)
```

**Notes:**
- The local model (TinyLlama-1.1B-Chat) loads on startup (~1-2 minutes)
- Runs on CPU (no GPU required)
- Uses ~2-3GB RAM
- Prompts are automatically wrapped in chat format
- Streaming uses background threads for non-blocking generation

## Error Handling

The API provides unified error responses in JSON format for better debugging and error tracking.

### Error Response Format

All errors follow this consistent structure:

```json
{
  "error": {
    "code": "ERROR_CODE",
    "message": "Human-readable error message",
    "status": 422,
    "field": "field_name",
    "details": {
      "additional": "information"
    }
  }
}
```

### Error Codes

| Code | HTTP Status | Description |
|------|-------------|-------------|
| `INVALID_JSON` | 422 | Malformed JSON request body |
| `MISSING_REQUIRED_FIELD` | 422 | Required field missing from request |
| `INVALID_TYPE` | 422 | Field has wrong data type |
| `INVALID_VALUE` | 422 | Field value is invalid or out of range |
| `NOT_FOUND` | 404 | Endpoint or resource not found |
| `RATE_LIMIT_EXCEEDED` | 429 | Too many requests, slow down |
| `INTERNAL_ERROR` | 500 | Server-side error occurred |
| `TIMEOUT` | 504 | Request timed out |

### Input Validation

All endpoints validate input parameters:

**OpenAI Routes (`/generate`):**
- `prompt`: Max 1000 characters
- `temperature`: 0-2
- `top_p`: 0-1
- `max_tokens`: 1-16384

**Local Routes (`/local/generate`):**
- `prompt`: Non-empty, max 1000 characters
- `temperature`: 0-2
- `top_p`: 0-1
- `max_tokens`: 1-2000

**Example Error Response:**

```json
{
  "error": {
    "code": "INVALID_VALUE",
    "message": "Temperature must be between 0 and 2",
    "status": 422,
    "field": "temperature",
    "details": {
      "min": 0,
      "max": 2,
      "received": 3.5
    }
  }
}
```

## API Security & Reliability

### Rate Limiting

Rate limits protect the API from abuse and ensure fair usage:

| Endpoint | Rate Limit | Window |
|----------|-----------|--------|
| `/generate` (OpenAI) | 10 requests | per minute |
| `/local/generate` | 20 requests | per minute |
| `/health` | 30 requests | per minute |

**When rate limited (HTTP 429):**

```json
{
  "error": "Rate limit exceeded: 10 per 1 minute"
}
```

**Best Practices:**
- Implement exponential backoff when receiving 429 errors
- Cache responses when possible
- Use streaming for long-running requests

### Timeout & Retry

OpenAI API calls include automatic timeout and retry mechanisms:

**Timeout Configuration:**
- Connection timeout: 10 seconds
- Read timeout: 20 seconds
- Write timeout: 10 seconds

**Retry Strategy:**
- Maximum retries: 3 attempts
- Strategy: Exponential backoff (1s ‚Üí 2s ‚Üí 4s ‚Üí 8s)
- Retries on: Network errors, timeouts, server errors (5xx)

**Example retry sequence:**

```
Attempt 1: Failed (timeout) ‚Üí Wait 1 second
Attempt 2: Failed (connection error) ‚Üí Wait 2 seconds
Attempt 3: Success ‚úì
```

If all retries fail, returns HTTP 500 with error details.

## Docker Deployment

### Using Docker Compose

```bash
# Start all services
docker compose -f infra/docker-compose.yml up -d --build

# View logs
docker compose -f infra/docker-compose.yml logs -f

# Stop services
docker compose -f infra/docker-compose.yml down
```

### Using build script (Windows)

```powershell
# Start services
.\build.ps1 compose-up

# Stop services
.\build.ps1 compose-down
```

## Configuration

### Environment Variables

Create a `.env` file in the project root:

```env
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# OpenAI Configuration
OPENAI_API_KEY=your_api_key_here

# Model Configuration
DEFAULT_MODEL=gpt-4o-mini
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=1000

# Application Settings
ENV=development
DEBUG=true
```

### Model Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `prompt` | string | required | Input text prompt |
| `temperature` | float | 0.0 | Controls randomness (0.0-2.0) |
| `top_p` | float | 0.9 | Nucleus sampling parameter |
| `max_tokens` | integer | 500 | Maximum response length |

## Development

### Project Commands

**Windows (PowerShell):**

```powershell
# Start development services
.\build.ps1 dev-services

# Start API
.\build.ps1 dev-api

# Run tests
.\build.ps1 smoke

# View help
.\build.ps1 help
```

**Linux/Mac (Makefile):**

```bash
# Start Docker services
make compose-up

# Stop Docker services
make compose-down

# Run smoke tests
make smoke
```

### Local Model Setup (Optional)

To use local open-source models (Qwen, LLaMA, etc.):

1. Install additional dependencies:
```bash
pip install -r requirements-local.txt
```

2. Download model files (example with Hugging Face):
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat")
```

3. Integrate into your routes (see `apps/api/src/routes/`)

### Adding New Routes

Create new route files in `apps/api/src/routes/`:

```python
from fastapi import APIRouter

router = APIRouter()

@router.get("/your-endpoint")
async def your_function():
    return {"message": "Hello"}
```

Register in `main.py`:

```python
from .routes import your_routes

app.include_router(your_routes.router)
```

## Troubleshooting

### Common Issues

**1. ModuleNotFoundError**

Make sure virtual environment is activated:
```bash
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac
```

**2. CORS Errors**

CORS is enabled by default for all origins in development. For production, update `apps/api/src/main.py`:

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://yourdomain.com"],  # Specify allowed origins
    ...
)
```

**3. Port Already in Use**

Change the port in the uvicorn command:
```bash
uvicorn apps.api.src.main:app --reload --port 8001
```

**4. Local Model Returns Empty Response**

If the local model endpoint `/local/generate` returns empty responses, check:

- **Ensure Chat Format**: TinyLlama-Chat expects specific format. The API automatically wraps prompts, but if manually testing, use:
  ```python
  prompt = """<|system|>
  You are a helpful assistant.</s>
  <|user|>
  Your question here</s>
  <|assistant|>
  """
  ```

- **Check pad_token**: Make sure `tokenizer.pad_token` is set (automatically handled in code):
  ```python
  if tokenizer.pad_token is None:
      tokenizer.pad_token = tokenizer.eos_token
  ```

- **Use Deterministic Generation**: Set `temperature=0.0` for consistent results (default behavior)

**5. PowerShell curl Testing Issues**

When testing with curl on Windows PowerShell, JSON escaping can cause issues:

**Problem:**
```powershell
# This may fail due to escaping issues
curl.exe -d '{\"prompt\": \"test\", \"max_tokens\": 50}'
```

**Solution 1: Use JSON file (Recommended)**
```powershell
# Create test_local.json:
{
  "prompt": "What is Python?",
  "max_tokens": 50
}

# Test with:
curl.exe -X POST "http://localhost:8000/local/generate" -H "Content-Type: application/json" -d "@test_local.json"
```

**Solution 2: Use PowerShell's Invoke-RestMethod**
```powershell
$body = @{
    prompt = "What is Python?"
    max_tokens = 50
} | ConvertTo-Json

Invoke-RestMethod -Uri "http://localhost:8000/local/generate" -Method Post -Body $body -ContentType "application/json"
```

**Solution 3: Use Swagger UI**

Navigate to `http://localhost:8000/docs` and test directly in the browser.

**6. localhost vs 127.0.0.1 Issues**

If `localhost` doesn't work but `127.0.0.1` does, start uvicorn with:

```bash
# Listen on all interfaces (IPv4 and IPv6)
uvicorn apps.api.src.main:app --reload --host 0.0.0.0 --port 8000
```

This ensures the service responds to both IPv4 and IPv6 addresses.

## Technologies

- **FastAPI**: Modern web framework for building APIs
- **Uvicorn**: ASGI server
- **OpenAI**: Cloud-based AI models
- **Transformers**: Hugging Face library for local model support
- **PyTorch**: Deep learning framework for model inference
- **TinyLlama**: Lightweight 1.1B parameter chat model
- **httpx**: HTTP client with timeout support
- **tenacity**: Retry mechanism with exponential backoff
- **slowapi**: Rate limiting middleware for FastAPI
- **Docker**: Containerization
- **Pydantic**: Data validation and settings management

## Roadmap

- [ ] RAG (Retrieval-Augmented Generation) service
- [ ] AI Agents framework
- [ ] Vector database integration
- [ ] Authentication & authorization
- [x] Rate limiting
- [ ] Caching layer
- [ ] Monitoring & logging
- [ ] Multi-model routing
- [x] Error handling with unified response format
- [x] Input validation
- [x] Timeout and retry mechanisms

---

**Made with ‚ù§Ô∏è using FastAPI and OpenAI**

